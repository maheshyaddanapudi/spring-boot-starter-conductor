springdoc.api-docs.path=/api-docs

spring.autoconfigure.exclude=org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration

loadSample=true

conductor.db.type=memory
#conductor.queue.type=redis_standalone

conductor.indexing.enabled=false

#Redis configuration details.
#format is host:port:rack separated by semicolon
#Auth is supported. Password is taken from host[0]. format: host:port:rack:password
#conductor.redis.hosts=host1:port:rack;host2:port:rack:host3:port:rack
conductor.redis.hosts=localhost:6379:us-east-1c

#namespace for the keys stored in Dynomite/Redis
conductor.redis.workflowNamespacePrefix=

#namespace prefix for the dyno queues
conductor.redis.queueNamespacePrefix=

#no. of threads allocated to dyno-queues
queues.dynomite.threads=10

# By default with dynomite, we want the repair service enabled
conductor.workflow-repair-service.enabled=true

#non-quorum port used to connect to local redis.  Used by dyno-queues
conductor.redis.queuesNonQuorumPort=22122

# For a single node dynomite or redis server, make sure the value below is set to same as rack specified in the "workflow.dynomite.cluster.hosts" property.
conductor.redis.availabilityZone=us-east-1c
#conductor.redis.maxIdleConnections=8
#conductor.redis.minIdleConnections=5
#conductor.redis.minEvictableIdleTimeMillis = 1800000
#conductor.redis.timeBetweenEvictionRunsMillis = -1L
#conductor.redis.testWhileIdle = false
#conductor.redis.numTestsPerEvictionRun = 3

#Transport address to elasticsearch
conductor.elasticsearch.url=localhost:9300

#Name of the elasticsearch cluster
conductor.elasticsearch.indexName=conductor

#Elasticsearch major release version.
conductor.elasticsearch.version=6
#conductor.elasticsearch.version=7

# Default event queue type to listen on for wait task
conductor.default-event-queue.type=sqs

#zookeeper
# conductor.zookeeper-lock.connectionString=host1.2181,host2:2181,host3:2181
# conductor.zookeeper-lock.sessionTimeoutMs
# conductor.zookeeper-lock.connectionTimeoutMs
# conductor.zookeeper-lock.namespace

#disable locking during workflow execution
conductor.app.workflow-execution-lock-enabled=false
conductor.workflow-execution-lock.type=noop_lock

#Redis cluster settings for locking module
# conductor.redis-lock.serverType=single
#Comma separated list of server nodes
# conductor.redis-lock.serverAddress=redis://127.0.0.1:6379
#Redis sentinel master name
# conductor.redis-lock.serverMasterName=master
# conductor.redis-lock.namespace

#Following properties set for using AMQP events and tasks with conductor:
#(To enable support of AMQP queues)
#conductor.event-queues.amqp.enabled=true

# Here are the settings with default values:
#conductor.event-queues.amqp.hosts=<rabbitmq serverip>
#conductor.event-queues.amqp.username=<username>
#conductor.event-queues.amqp.password=<password>

#conductor.event-queues.amqp.virtualHost=/
#conductor.event-queues.amqp.port=5672
#conductor.event-queues.amqp.useNio=false
#conductor.event-queues.amqp.batchSize=1
#conductor.event-queues.amqp.pollTimeDuration=100ms
#conductor.event-queues.amqp.queueType=classic
#conductor.event-queues.amqp.sequentialMsgProcessing=true
#conductor.event-queues.amqp.connectionTimeoutInMilliSecs=180000
#conductor.event-queues.amqp.networkRecoveryIntervalInMilliSecs=5000
#conductor.event-queues.amqp.requestHeartbeatTimeoutInSecs=30
#conductor.event-queues.amqp.handshakeTimeoutInMilliSecs=180000
#conductor.event-queues.amqp.maxChannelCount=5000
#conductor.event-queues.amqp.limit=50
#conductor.event-queues.amqp.duration=1000
#conductor.event-queues.amqp.retryType=REGULARINTERVALS

#conductor.event-queues.amqp.useExchange=true( exchange or queue)
#conductor.event-queues.amqp.listenerQueuePrefix=myqueue
# Use durable queue ?
#conductor.event-queues.amqp.durable=false
# Use exclusive queue ?
#conductor.event-queues.amqp.exclusive=false
# Enable support of priorities on queue. Set the max priority on message.
# Setting is ignored if the value is lower or equals to 0
#conductor.event-queues.amqp.maxPriority=-1

# To enable Workflow/Task Summary Input/Output JSON Serialization, use the following:
# conductor.app.summary-input-output-json-serialization.enabled=true

# Additional modules for metrics collection exposed to Prometheus (optional)
# conductor.metrics-prometheus.enabled=true

# Enable status listeners for workflows and tasks
#conductor.workflow-status-listener.type=workflow_publisher
#conductor.task-status-listener.type=task_publisher

# Webhook target URLs for workflow and task notifications
#conductor.status-notifier.notification.url=https://example.com/notifications
#conductor.status-notifier.notification.endpointWorkflow=/workflow-status
#conductor.status-notifier.notification.endpointTask=/task-status

# Specify subscribed task statuses for notifications
conductor.status-notifier.notification.subscribedTaskStatuses=COMPLETED,FAILED

# Optional fine-tuning properties for notification retries and timeouts
#conductor.status-notifier.notification.requestTimeoutMsConnect=100
#conductor.status-notifier.notification.requestRetryCount=3
#conductor.status-notifier.notification.requestRetryIntervalMs=50

# Enable Actuator endpoints for monitoring and observability
management.endpoints.web.exposure.include=health,info,prometheus
management.endpoint.health.show-details=always

# Disable workflow name validation (enable for production environments)
conductor.app.workflow.name-validation.enabled=false

# Additional modules for metrics collection exposed to Datadog (optional)
management.metrics.export.datadog.enabled=${conductor.metrics-datadog.enabled:false}
management.metrics.export.datadog.api-key=${conductor.metrics-datadog.api-key:}

management.endpoints.web.exposure.include=loggers
logging.level.com.netflix.dyno.queues=ERROR
